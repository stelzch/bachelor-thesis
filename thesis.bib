
@misc{noauthor_simd_nodate,
	title = {{SIMD} for C++ Developers},
	url = {http://const.me/articles/simd/simd.pdf},
	urldate = {2021-11-16},
	keywords = {{SIMD}},
	file = {simd.pdf:/home/christoph/Zotero/storage/J644N69C/simd.pdf:application/pdf},
}

@online{noauthor_intel_nodate,
	title = {Intel® Intrinsics Guide},
	url = {https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html},
	abstract = {Intel® Intrinsics Guide},
	titleaddon = {Intel},
	urldate = {2021-11-16},
	langid = {english},
	keywords = {{SIMD}},
}

@article{ahrens_algorithms_2020,
	title = {Algorithms for Efficient Reproducible Floating Point Summation},
	volume = {46},
	issn = {0098-3500, 1557-7295},
	url = {https://dl.acm.org/doi/10.1145/3389360},
	doi = {10.1145/3389360},
	abstract = {We define “reproducibility” as getting bitwise identical results from multiple runs of the same program, perhaps with different hardware resources or other changes that should not affect the answer. Many users depend on reproducibility for debugging or correctness. However, dynamic scheduling of parallel computing resources, combined with nonassociative floating point addition, makes reproducibility challenging even for summation, or operations like the {BLAS}. We describe a “reproducible accumulator” data structure (the “binned number”) and associated algorithms to reproducibly sum binary floating point numbers, independent of summation order. We use a subset of the {IEEE} Floating Point Standard 754-2008 and bitwise operations on the standard representations in memory. Our approach requires only one read-only pass over the data, and one reduction in parallel, using a 6-word reproducible accumulator (more words can be used for higher accuracy), enabling standard tiling optimization techniques. Summing
              n
              words with a 6-word reproducible accumulator requires approximately 9
              n
              floating point operations (arithmetic, comparison, and absolute value) and approximately 3
              n
              bitwise operations. The final error bound with a 6-word reproducible accumulator and our default settings can be up to 2
              29
              times smaller than the error bound for conventional (recursive) summation on ill-conditioned double-precision inputs.},
	pages = {1--49},
	number = {3},
	journaltitle = {{ACM} Transactions on Mathematical Software},
	shortjournal = {{ACM} Trans. Math. Softw.},
	author = {Ahrens, Peter and Demmel, James and Nguyen, Hong Diep},
	date = {2020-09-25},
	langid = {english},
	file = {Ahrens et al. - 2020 - Algorithms for Efficient Reproducible Floating Poi.pdf:/home/christoph/Zotero/storage/LLYPDG2U/Ahrens et al. - 2020 - Algorithms for Efficient Reproducible Floating Poi.pdf:application/pdf},
}

@article{rump_fast_2010,
	title = {Fast high precision summation},
	volume = {1},
	pages = {2--24},
	number = {1},
	journaltitle = {Nonlinear Theory and Its Applications, {IEICE}},
	author = {Rump, Siegfried M and Ogita, Takeshi and Oishi, Shin'ichi},
	date = {2010},
	note = {Publisher: The Institute of Electronics, Information and Communication Engineers},
	keywords = {Memory expansion, Reproducibility, Summation},
	file = {Rump et al. - 2010 - Fast high precision summation.pdf:/home/christoph/Zotero/storage/CH53KA7Q/Rump et al. - 2010 - Fast high precision summation.pdf:application/pdf},
}

@report{noauthor_ieee_nodate,
	title = {{IEEE} Standard for Floating-Point Arithmetic},
	url = {http://ieeexplore.ieee.org/document/4610935/},
	institution = {{IEEE}},
	urldate = {2021-10-01},
	doi = {10.1109/IEEESTD.2008.4610935},
	note = {{ISBN}: 9780738157528},
	file = {Volltext:/home/christoph/Zotero/storage/9DRU2QTC/IEEE Standard for Floating-Point Arithmetic.pdf:application/pdf},
}

@inproceedings{demmel_fast_2013,
	title = {Fast reproducible floating-point summation},
	pages = {163--172},
	booktitle = {2013 {IEEE} 21st Symposium on Computer Arithmetic},
	publisher = {{IEEE}},
	author = {Demmel, James and Nguyen, Hong Diep},
	date = {2013},
	keywords = {Memory expansion},
	file = {Demmel und Nguyen - 2013 - Fast reproducible floating-point summation.pdf:/home/christoph/Zotero/storage/LA8W2A6P/Demmel und Nguyen - 2013 - Fast reproducible floating-point summation.pdf:application/pdf},
}

@article{demmel_parallel_2015,
	title = {Parallel Reproducible Summation},
	volume = {64},
	issn = {0018-9340},
	url = {http://ieeexplore.ieee.org/document/6875899/},
	doi = {10.1109/TC.2014.2345391},
	pages = {2060--2070},
	number = {7},
	journaltitle = {{IEEE} Transactions on Computers},
	shortjournal = {{IEEE} Trans. Comput.},
	author = {Demmel, James and Nguyen, Hong Diep},
	urldate = {2021-10-01},
	date = {2015-07-01},
	keywords = {Memory expansion},
	file = {Demmel und Nguyen - 2015 - Parallel Reproducible Summation.pdf:/home/christoph/Zotero/storage/YRZTK3RH/Demmel und Nguyen - 2015 - Parallel Reproducible Summation.pdf:application/pdf},
}

@article{kahan_pracniques_1965,
	title = {Pracniques: further remarks on reducing truncation errors},
	volume = {8},
	pages = {40},
	number = {1},
	journaltitle = {Communications of the {ACM}},
	author = {Kahan, William},
	date = {1965},
	keywords = {Memory expansion, Kahan summation},
	file = {363707.363723.pdf:/home/christoph/Zotero/storage/A9EE4GTQ/363707.363723.pdf:application/pdf},
}

@article{robey_search_2011,
	title = {In search of numerical consistency in parallel programming},
	volume = {37},
	issn = {0167-8191},
	url = {https://www.sciencedirect.com/science/article/pii/S0167819111000238},
	doi = {10.1016/j.parco.2011.02.009},
	abstract = {We present methods that can dramatically improve numerical consistency for parallel calculations across varying numbers of processors. By calculating global sums with enhanced precision techniques based on Kahan or Knuth summations, the consistency of the numerical results can be greatly improved with minimal memory and computational cost. This study assesses the value of the enhanced numerical consistency in the context of general finite difference or finite volume calculations.},
	pages = {217--229},
	number = {4},
	journaltitle = {Parallel Computing},
	shortjournal = {Parallel Computing},
	author = {Robey, Robert W. and Robey, Jonathan M. and Aulwes, Rob},
	urldate = {2021-10-01},
	date = {2011-04-01},
	langid = {english},
	keywords = {Memory expansion, Reproducibility, Kahan summation, Finite difference, Finite volume, Knuth summation, Numerical consistency, Parallel programming},
	file = {Robey et al. - 2011 - In search of numerical consistency in parallel pro.pdf:/home/christoph/Zotero/storage/CILUT9ZQ/Robey et al. - 2011 - In search of numerical consistency in parallel pro.pdf:application/pdf},
}

@inproceedings{villa_effects_2009,
	title = {Effects of floating-point non-associativity on numerical computations on massively multithreaded systems},
	volume = {3},
	url = {http://www.sci.utah.edu/~beiwang/teaching/cs6210-fall-2016/nonassociativity.pdf},
	booktitle = {Proceedings of Cray User Group Meeting ({CUG})},
	author = {Villa, Oreste and Chavarria-Miranda, Daniel and Gurumoorthi, Vidhya and Marquéz, Andrés and Krishnamoorthy, Sriram},
	date = {2009},
	file = {nonassociativity.pdf:/home/christoph/Zotero/storage/X8XW9UA9/nonassociativity.pdf:application/pdf},
}

@unpublished{pollard_statistical_2020,
	title = {A Statistical Analysis of Error in {MPI} Reduction Operations},
	url = {https://sampollard.github.io/research/artifacts/pollard_correctness20_presentation.pdf},
	author = {Pollard, Samuel D. and Norris, Boyana},
	date = {2020-11-11},
	file = {pollard_correctness20_presentation.pdf:/home/christoph/Zotero/storage/N3N6KKA7/pollard_correctness20_presentation.pdf:application/pdf},
}

@inproceedings{pollard_statistical_2020-1,
	location = {{GA}, {USA}},
	title = {A Statistical Analysis of Error in {MPI} Reduction Operations},
	isbn = {978-0-7381-1044-8},
	url = {https://ieeexplore.ieee.org/document/9296938/},
	doi = {10.1109/Correctness51934.2020.00011},
	eventtitle = {2020 {IEEE}/{ACM} 4th International Workshop on Software Correctness for {HPC} Applications (Correctness)},
	pages = {49--57},
	booktitle = {2020 {IEEE}/{ACM} 4th International Workshop on Software Correctness for {HPC} Applications (Correctness)},
	publisher = {{IEEE}},
	author = {Pollard, Samuel D. and Norris, Boyana},
	urldate = {2021-09-28},
	date = {2020-11},
	file = {Volltext:/home/christoph/Zotero/storage/F7QMJA3V/Pollard und Norris - 2020 - A Statistical Analysis of Error in MPI Reduction O.pdf:application/pdf},
}

@article{collange_numerical_2015,
	title = {Numerical reproducibility for the parallel reduction on multi-and many-core architectures},
	volume = {49},
	url = {https://www.sciencedirect.com/science/article/pii/S0167819115001155?via%3Dihub},
	pages = {83--97},
	journaltitle = {Parallel Computing},
	author = {Collange, Sylvain and Defour, David and Graillat, Stef and Iakymchuk, Roman},
	date = {2015},
	note = {Publisher: Elsevier},
	file = {Collange et al. - 2015 - Numerical reproducibility for the parallel reducti.pdf:/home/christoph/Zotero/storage/5AUVHPRH/Collange et al. - 2015 - Numerical reproducibility for the parallel reducti.pdf:application/pdf},
}

@article{hunold_mpi_2015,
	title = {{MPI} benchmarking revisited: Experimental design and reproducibility},
	abstract = {The Message Passing Interface ({MPI}) is the prevalent programming model used on today’s supercomputers. Therefore, {MPIlibrary} developers are looking for the best possible performance (shortest run-time) of individual {MPI} functions across many differentsupercomputer architectures. Several {MPI} benchmark suites have been developed to assess the performance of {MPI} implementations.Unfortunately, the outcome of these benchmarks is often neither reproducible nor statistically sound. To overcome these issues, we showwhich experimental factors have an impact on the run-time of blocking collective {MPI} operations and how to control them. We address theproblem of process and clock synchronization in {MPI} benchmarks. Finally, we present a new experimental method that allows us to obtainreproducible and statistically sound {MPI} measurements},
	journaltitle = {{arXiv} preprint {arXiv}:1505.07734},
	author = {Hunold, Sascha and Carpen-Amarie, Alexandra},
	date = {2015},
	file = {Hunold und Carpen-Amarie - 2015 - MPI benchmarking revisited Experimental design an.pdf:/home/christoph/Zotero/storage/BSKN84WY/Hunold und Carpen-Amarie - 2015 - MPI benchmarking revisited Experimental design an.pdf:application/pdf},
}

@inproceedings{balaji_reproducibility_2013,
	title = {On the reproducibility of {MPI} reduction operations},
	pages = {407--414},
	booktitle = {2013 {IEEE} 10th International Conference on High Performance Computing and Communications \& 2013 {IEEE} International Conference on Embedded and Ubiquitous Computing},
	publisher = {{IEEE}},
	author = {Balaji, Pavan and Kimpe, Dries},
	date = {2013},
	file = {Balaji und Kimpe - On the Reproducibility of MPI Reduction Operations.pdf:/home/christoph/Zotero/storage/N3SB3TD7/Balaji und Kimpe - On the Reproducibility of MPI Reduction Operations.pdf:application/pdf},
}

@online{noauthor_introduction_nodate,
	title = {An Introduction to {GCC} Compiler Intrinsics in Vector Processing {\textbar} Linux Journal},
	url = {https://www.linuxjournal.com/content/introduction-gcc-compiler-intrinsics-vector-processing},
	urldate = {2021-12-03},
	keywords = {{SIMD}},
	file = {An Introduction to GCC Compiler Intrinsics in Vector Processing | Linux Journal:/home/christoph/Zotero/storage/9JZ8FVVW/introduction-gcc-compiler-intrinsics-vector-processing.html:text/html},
}

@online{noauthor_scalability_2018,
	title = {Scalability: strong and weak scaling},
	url = {https://www.kth.se/blogs/pdc/2018/11/scalability-strong-and-weak-scaling/},
	shorttitle = {Scalability},
	abstract = {High performance computing ({HPC}) clusters are able to solve big problems using a large number of processors. This is also known as parallel computing, where many processors work simultaneously to p…},
	titleaddon = {{PDC} Blog},
	urldate = {2021-12-10},
	date = {2018-11-09},
	langid = {british},
	file = {Snapshot:/home/christoph/Zotero/storage/PTGRBJUS/scalability-strong-and-weak-scaling.html:text/html},
}

@article{hubner_load-balance_nodate,
	title = {Load-Balance and Fault-Tolerance for Massively Parallel Phylogenetic Inference},
	pages = {115},
	author = {Hübner, Klaus Lukas},
	langid = {english},
	file = {Hübner - Load-Balance and Fault-Tolerance for Massively Par.pdf:/home/christoph/Zotero/storage/VWI7Z6W7/Hübner - Load-Balance and Fault-Tolerance for Massively Par.pdf:application/pdf},
}

@incollection{hutchison_parallel_2006,
	location = {Berlin, Heidelberg},
	title = {Parallel Prefix (Scan) Algorithms for {MPI}},
	volume = {4192},
	isbn = {978-3-540-39110-4 978-3-540-39112-8},
	url = {http://link.springer.com/10.1007/11846802_15},
	abstract = {We describe and experimentally compare three theoretically well-known algorithms for the parallel preﬁx (or scan, in {MPI} terms) operation, and give a presumably novel, doubly-pipelined implementation of the in-order binary tree parallel preﬁx algorithm. Bidirectional interconnects can beneﬁt from this implementation. We present results from a 32 node {AMD} Cluster with Myrinet 2000 and a 72-node {SX}-8 parallel vector system. On both systems, we observe improvements by more than a factor two over the straight-forward binomial-tree algorithm found in many {MPI} implementations. We also discuss adapting the algorithms to clusters of {SMP} nodes.},
	pages = {49--57},
	booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	publisher = {Springer Berlin Heidelberg},
	author = {Sanders, Peter and Träff, Jesper Larsson},
	editor = {Mohr, Bernd and Träff, Jesper Larsson and Worringen, Joachim and Dongarra, Jack},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2021-12-17},
	date = {2006},
	langid = {english},
	doi = {10.1007/11846802_15},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Sanders und Träff - 2006 - Parallel Prefix (Scan) Algorithms for MPI.pdf:/home/christoph/Zotero/storage/SFA5MFUA/Sanders und Träff - 2006 - Parallel Prefix (Scan) Algorithms for MPI.pdf:application/pdf},
}

@article{patarasuk_bandwidth_2009,
	title = {Bandwidth optimal all-reduce algorithms for clusters of workstations},
	volume = {69},
	issn = {0743-7315},
	url = {https://www.sciencedirect.com/science/article/pii/S0743731508001767},
	doi = {10.1016/j.jpdc.2008.09.002},
	abstract = {We consider an efficient realization of the all-reduce operation with large data sizes in cluster environments, under the assumption that the reduce operator is associative and commutative. We derive a tight lower bound of the amount of data that must be communicated in order to complete this operation and propose a ring-based algorithm that only requires tree connectivity to achieve bandwidth optimality. Unlike the widely used butterfly-like all-reduce algorithm that incurs network contention in {SMP}/multi-core clusters, the proposed algorithm can achieve contention-free communication in almost all contemporary clusters, including {SMP}/multi-core clusters and Ethernet switched clusters with multiple switches. We demonstrate that the proposed algorithm is more efficient than other algorithms on clusters with different nodal architectures and networking technologies when the data size is sufficiently large.},
	pages = {117--124},
	number = {2},
	journaltitle = {Journal of Parallel and Distributed Computing},
	shortjournal = {Journal of Parallel and Distributed Computing},
	author = {Patarasuk, Pitch and Yuan, Xin},
	urldate = {2021-12-17},
	date = {2009-02-01},
	langid = {english},
	keywords = {All-reduce, Cluster of workstations, Collective communication, Tree topology},
	file = {Eingereichte Version:/home/christoph/Zotero/storage/62QCML2I/Patarasuk und Yuan - 2009 - Bandwidth optimal all-reduce algorithms for cluste.pdf:application/pdf},
}

@online{konstantin_improving_2020,
	title = {Improving performance with {SIMD} intrinsics in three use cases},
	url = {https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/},
	abstract = {Many developers write software that’s performance sensitive. After all, that’s one of the major reasons why we still pick C or C++ language these days. When done right, supplementing C or C++ code with vector intrinsics is exceptionally good for performance.},
	titleaddon = {Stack Overflow Blog},
	author = {{Konstantin}},
	urldate = {2022-01-12},
	date = {2020-07-08},
	langid = {american},
	keywords = {{SIMD}},
	file = {Snapshot:/home/christoph/Zotero/storage/283NJH8C/improving-performance-with-simd-intrinsics-in-three-use-cases.html:text/html},
}

@article{curtsinger_coz_2015,
	title = {Coz: Finding Code that Counts with Causal Profiling},
	url = {http://arxiv.org/abs/1608.03676},
	doi = {10.1145/2815400.2815409},
	shorttitle = {Coz},
	abstract = {Improving performance is a central concern for software developers. To locate optimization opportunities, developers rely on software proﬁlers. However, these proﬁlers only report where programs spent their time: optimizing that code may have no impact on performance. Past proﬁlers thus both waste developer time and make it difﬁcult for them to uncover signiﬁcant optimization opportunities.},
	pages = {184--197},
	journaltitle = {Proceedings of the 25th Symposium on Operating Systems Principles},
	author = {Curtsinger, Charlie and Berger, Emery D.},
	urldate = {2021-12-20},
	date = {2015-10-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1608.03676},
	keywords = {C.4, Computer Science - Performance, D.4.8},
	file = {Curtsinger and Berger - 2015 - Coz Finding Code that Counts with Causal Profilin.pdf:/home/christoph/Zotero/storage/WVMIGH8B/Curtsinger and Berger - 2015 - Coz Finding Code that Counts with Causal Profilin.pdf:application/pdf},
}

@video{strange_loop_conference_performance_2019,
	title = {"Performance Matters" by Emery Berger},
	url = {https://www.youtube.com/watch?v=r-TLSBdHe1A},
	abstract = {Performance clearly matters to users. For example, the most common software update on the {AppStore} is "Bug fixes and performance enhancements." Now that Moore's Law has ended, programmers have to work hard to get high performance for their applications. But why is performance hard to deliver?

I will first explain why current approaches to evaluating and optimizing performance don't work, especially on modern hardware and for modern applications. I then present two systems that address these challenges. Stabilizer is a tool that enables statistically sound performance evaluation, making it possible to understand the impact of optimizations and conclude things like the fact that the -O2 and -O3 optimization levels are indistinguishable from noise (sadly true).

Since compiler optimizations have run out of steam, we need better profiling support, especially for modern concurrent, multi-threaded applications. Coz is a new "causal profiler" that lets programmers optimize for throughput or latency, and which pinpoints and accurately predicts the impact of optimizations. Coz's approach unlocks previously unknown optimization opportunities. Guided by Coz, we improved the performance of Memcached (9\%), {SQLite} (25\%), and accelerated six other applications by as much as 68\%; in most cases, this involved modifying less than 10 lines of code and took under half an hour (without any prior understanding of the programs!). Coz now ships as part of standard Linux distros (apt install coz-profiler).

Emery Berger
University of Massachusetts Amherst
@emeryberger

Emery Berger is a Professor in the College of Information and Computer Sciences at the University of Massachusetts Amherst, the flagship campus of the {UMass} system. He graduated with a Ph.D. in Computer Science from the University of Texas at Austin in 2002. Professor Berger has been a Visiting Scientist at Microsoft Research (where he is currently on sabbatical), the University of Washington, and at the Universitat Politècnica de Catalunya ({UPC}) / Barcelona Supercomputing Center ({BSC}). Professor Berger's research spans programming languages, runtime systems, and operating systems, with a particular focus on systems that transparently improve reliability, security, and performance. He and his collaborators have created a number of influential software systems including Hoard, a fast and scalable memory manager that accelerates multithreaded applications (used by companies including British Telecom, Cisco, Crédit Suisse, Reuters, Royal Bank of Canada, {SAP}, and Tata, and on which the Mac {OS} X memory manager is based); {DieHard}, an error-avoiding memory manager that directly influenced the design of the Windows 7 Fault-Tolerant Heap; and {DieHarder}, a secure memory manager that was an inspiration for hardening changes made to the Windows 8 heap. His honors include a Microsoft Research Fellowship, an {NSF} {CAREER} Award, a Lilly Teaching Fellowship, the Distinguished Artifact Award for {PLDI} 2014, Most Influential Paper Awards at {OOPSLA}, {PLDI}, and {ASPLOS}, three {CACM} Research Highlights, a Google Research Award, a Microsoft {SEIF} Award, and Best Paper Awards at {FAST}, {OOPSLA}, and {SOSP}; he was named an {ACM} Distinguished Member in 2018. Professor Berger is currently serving his second term as an elected member of the {SIGPLAN} Executive Committee; he served for a decade (2007-2017) as Associate Editor of the {ACM} Transactions on Programming Languages and Systems, and was Program Chair for {PLDI} 2016.},
	author = {{Strange Loop Conference}},
	urldate = {2021-12-20},
	date = {2019-09-15},
	keywords = {benchmarking},
}

@book{knuth_texbook_1986,
	location = {Reading, Mass},
	title = {The {TeXbook}},
	isbn = {978-0-201-13447-6 978-0-201-13448-3},
	series = {Computers \& typesetting},
	pagetotal = {483},
	number = {A},
	publisher = {Addison-Wesley},
	author = {Knuth, Donald Ervin},
	date = {1986},
	langid = {english},
	keywords = {Computer programs, Computerized typesetting, Mathematics printing, {TeX} (Computer file)},
	file = {Knuth und Knuth - 1986 - The TeXbook.pdf:/home/christoph/Zotero/storage/HWTZA4Q3/Knuth und Knuth - 1986 - The TeXbook.pdf:application/pdf},
}

@article{kording_ten_nodate,
	title = {Ten simple rules for structuring papers},
	abstract = {Good scientific writing is essential to career development and to the progress of science. A well-structured manuscript allows readers and reviewers to get excited about the subject matter, to understand and verify the paper’s contributions, and to integrate them into a broader context. However, many scientists struggle with producing high-quality manuscripts and are typically given little training in paper writing. Focusing on how readers consume information, we present a set of 10 simple rules to help you get across the main idea of your paper. These rules are designed to make your paper more influential and the process of writing more efficient and pleasurable.},
	pages = {12},
	author = {Kording, Konrad and Mensh, Brett},
	langid = {english},
	file = {Ten simple rules for structuring papers.pdf:/home/christoph/Zotero/storage/3VFV5C5R/Ten simple rules for structuring papers.pdf:application/pdf},
}

@article{stamatakis_raxml_2014,
	title = {{RAxML} version 8: a tool for phylogenetic analysis and post-analysis of large phylogenies},
	volume = {30},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btu033},
	doi = {10.1093/bioinformatics/btu033},
	shorttitle = {{RAxML} version 8},
	abstract = {Motivation: Phylogenies are increasingly used in all fields of medical and biological research. Moreover, because of the next-generation sequencing revolution, datasets used for conducting phylogenetic analyses grow at an unprecedented pace. {RAxML} (Randomized Axelerated Maximum Likelihood) is a popular program for phylogenetic analyses of large datasets under maximum likelihood. Since the last {RAxML} paper in 2006, it has been continuously maintained and extended to accommodate the increasingly growing input datasets and to serve the needs of the user community.Results: I present some of the most notable new features and extensions of {RAxML}, such as a substantial extension of substitution models and supported data types, the introduction of {SSE}3, {AVX} and {AVX}2 vector intrinsics, techniques for reducing the memory requirements of the code and a plethora of operations for conducting post-analyses on sets of trees. In addition, an up-to-date 50-page user manual covering all new {RAxML} options is available.Availability and implementation: The code is available under {GNU} {GPL} at https://github.com/stamatak/standard-{RAxML}.Contact:alexandros.stamatakis@h-its.{orgSupplementary} information:Supplementary data are available at Bioinformatics online.},
	pages = {1312--1313},
	number = {9},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Stamatakis, Alexandros},
	urldate = {2022-01-28},
	date = {2014-05-01},
	file = {Full Text PDF:/home/christoph/Zotero/storage/K9GWF7PW/Stamatakis - 2014 - RAxML version 8 a tool for phylogenetic analysis .pdf:application/pdf;Snapshot:/home/christoph/Zotero/storage/JJNCXNUR/238053.html:text/html},
}

@article{baker_reproducibility_2016,
	title = {Reproducibility crisis},
	volume = {533},
	pages = {353--66},
	number = {26},
	journaltitle = {Nature},
	author = {Baker, Monya},
	date = {2016},
	keywords = {Reproducibility},
	file = {Baker - 2016 - Reproducibility crisis.pdf:/home/christoph/Zotero/storage/JKUNTYFE/Baker - 2016 - Reproducibility crisis.pdf:application/pdf},
}

@article{sandve_ten_2013,
	title = {Ten Simple Rules for Reproducible Computational Research},
	volume = {9},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285},
	doi = {10.1371/journal.pcbi.1003285},
	pages = {e1003285},
	number = {10},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
	urldate = {2022-01-31},
	date = {2013-10-24},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Reproducibility, Archives, Computer and information sciences, Computer applications, Genome analysis, Habits, Replication studies, Source code},
	file = {Full Text PDF:/home/christoph/Zotero/storage/HU7AFFCA/Sandve et al. - 2013 - Ten Simple Rules for Reproducible Computational Re.pdf:application/pdf;Snapshot:/home/christoph/Zotero/storage/FJHBKJCC/article.html:text/html},
}

@article{milkowski_replicability_2018,
	title = {Replicability or reproducibility? On the replication crisis in computational neuroscience and sharing only relevant detail},
	volume = {45},
	issn = {1573-6873},
	url = {https://doi.org/10.1007/s10827-018-0702-z},
	doi = {10.1007/s10827-018-0702-z},
	shorttitle = {Replicability or reproducibility?},
	abstract = {Replicability and reproducibility of computational models has been somewhat understudied by “the replication movement.” In this paper, we draw on methodological studies into the replicability of psychological experiments and on the mechanistic account of explanation to analyze the functions of model replications and model reproductions in computational neuroscience. We contend that model replicability, or independent researchers' ability to obtain the same output using original code and data, and model reproducibility, or independent researchers' ability to recreate a model without original code, serve different functions and fail for different reasons. This means that measures designed to improve model replicability may not enhance (and, in some cases, may actually damage) model reproducibility. We claim that although both are undesirable, low model reproducibility poses more of a threat to long-term scientific progress than low model replicability. In our opinion, low model reproducibility stems mostly from authors' omitting to provide crucial information in scientific papers and we stress that sharing all computer code and data is not a solution. Reports of computational studies should remain selective and include all and only relevant bits of code.},
	pages = {163--172},
	number = {3},
	journaltitle = {Journal of Computational Neuroscience},
	shortjournal = {J Comput Neurosci},
	author = {Miłkowski, Marcin and Hensel, Witold M. and Hohol, Mateusz},
	urldate = {2022-01-31},
	date = {2018-12-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/christoph/Zotero/storage/MDD4CUKN/Miłkowski et al. - 2018 - Replicability or reproducibility On the replicati.pdf:application/pdf},
}

@article{kozlov_raxml-ng_2019,
	title = {{RAxML}-{NG}: a fast, scalable and user-friendly tool for maximum likelihood phylogenetic inference},
	volume = {35},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btz305},
	doi = {10.1093/bioinformatics/btz305},
	shorttitle = {{RAxML}-{NG}},
	abstract = {Phylogenies are important for fundamental biological research, but also have numerous applications in biotechnology, agriculture and medicine. Finding the optimal tree under the popular maximum likelihood ({ML}) criterion is known to be {NP}-hard. Thus, highly optimized and scalable codes are needed to analyze constantly growing empirical datasets.We present {RAxML}-{NG}, a from-scratch re-implementation of the established greedy tree search algorithm of {RAxML}/{ExaML}. {RAxML}-{NG} offers improved accuracy, flexibility, speed, scalability, and usability compared with {RAxML}/{ExaML}. On taxon-rich datasets, {RAxML}-{NG} typically finds higher-scoring trees than {IQTree}, an increasingly popular recent tool for {ML}-based phylogenetic inference (although {IQ}-Tree shows better stability). Finally, {RAxML}-{NG} introduces several new features, such as the detection of terraces in tree space and the recently introduced transfer bootstrap support metric.The code is available under {GNU} {GPL} at https://github.com/amkozlov/raxml-ng. {RAxML}-{NG} web service (maintained by Vital-{IT}) is available at https://raxml-ng.vital-it.ch/.Supplementary data are available at Bioinformatics online.},
	pages = {4453--4455},
	number = {21},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Kozlov, Alexey M and Darriba, Diego and Flouri, Tomáš and Morel, Benoit and Stamatakis, Alexandros},
	date = {2019-11-01},
	file = {Full Text PDF:/home/christoph/Zotero/storage/7SP56GWY/Kozlov et al. - 2019 - RAxML-NG a fast, scalable and user-friendly tool .pdf:application/pdf;Snapshot:/home/christoph/Zotero/storage/PEMVMW3T/5487384.html:text/html},
}

@article{mcdougal_reproducibility_2016,
	title = {Reproducibility in Computational Neuroscience Models and Simulations},
	volume = {63},
	issn = {1558-2531},
	doi = {10.1109/TBME.2016.2539602},
	abstract = {Objective: Like all scientific research, computational neuroscience research must be reproducible. Big data science, including simulation research, cannot depend exclusively on journal articles as the method to provide the sharing and transparency required for reproducibility. Methods: Ensuring model reproducibility requires the use of multiple standard software practices and tools, including version control, strong commenting and documentation, and code modularity. Results: Building on these standard practices, model-sharing sites and tools have been developed that fit into several categories: 1) standardized neural simulators; 2) shared computational resources; 3) declarative model descriptors, ontologies, and standardized annotations; and 4) model-sharing repositories and sharing standards. Conclusion: A number of complementary innovations have been proposed to enhance sharing, transparency, and reproducibility. The individual user can be encouraged to make use of version control, commenting, documentation, and modularity in development of models. The community can help by requiring model sharing as a condition of publication and funding. Significance: Model management will become increasingly important as multiscale models become larger, more detailed, and correspondingly more difficult to manage by any single investigator or single laboratory. Additional big data management complexity will come as the models become more useful in interpreting experiments, thus increasing the need to ensure clear alignment between modeling data, both parameters and results, and experiment.},
	pages = {2021--2035},
	number = {10},
	journaltitle = {{IEEE} Transactions on Biomedical Engineering},
	author = {{McDougal}, Robert A. and Bulanova, Anna S. and Lytton, William W.},
	date = {2016-10},
	note = {Conference Name: {IEEE} Transactions on Biomedical Engineering},
	keywords = {Annotation, Biological system modeling, Computational modeling, computational neuroscience, Data models, Mathematical model, model sharing, Neuroscience, Numerical models, reproducibility, simulator, Software},
	file = {Akzeptierte Version:/home/christoph/Zotero/storage/XTQCQZYE/McDougal et al. - 2016 - Reproducibility in Computational Neuroscience Mode.pdf:application/pdf;IEEE Xplore Abstract Record:/home/christoph/Zotero/storage/Q7ZMY8LY/7428867.html:text/html},
}

@article{fanelli_opinion_2018,
	title = {Opinion: Is science really facing a reproducibility crisis, and do we need it to?},
	volume = {115},
	rights = {© 2018 . http://www.pnas.org/site/aboutpnas/licenses.{xhtmlPublished} under the {PNAS} license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/11/2628},
	doi = {10.1073/pnas.1708272114},
	shorttitle = {Opinion},
	abstract = {Efforts to improve the reproducibility and integrity of science are typically justified by a narrative of crisis, according to which most published results are unreliable due to growing problems with research and publication practices. This article provides an overview of recent evidence suggesting that this narrative is mistaken, and argues that a narrative of epochal changes and empowerment of scientists would be more accurate, inspiring, and compelling.},
	pages = {2628--2631},
	number = {11},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Fanelli, Daniele},
	urldate = {2022-01-31},
	date = {2018-03-13},
	langid = {english},
	pmid = {29531051},
	note = {Publisher: National Academy of Sciences
Section: Colloquium {OPINION}},
	keywords = {bias, crisis, integrity, misconduct, reproducible research},
	file = {Full Text PDF:/home/christoph/Zotero/storage/GZED4WIQ/Fanelli - 2018 - Opinion Is science really facing a reproducibilit.pdf:application/pdf;Snapshot:/home/christoph/Zotero/storage/7RLF45HL/2628.html:text/html},
}

@article{darriba_state_2018,
	title = {The State of Software for Evolutionary Biology},
	volume = {35},
	issn = {0737-4038},
	url = {https://doi.org/10.1093/molbev/msy014},
	doi = {10.1093/molbev/msy014},
	abstract = {With Next Generation Sequencing data being routinely used, evolutionary biology is transforming into a computational science. Thus, researchers have to rely on a growing number of increasingly complex software. All widely used core tools in the field have grown considerably, in terms of the number of features as well as lines of code and consequently, also with respect to software complexity. A topic that has received little attention is the software engineering quality of widely used core analysis tools. Software developers appear to rarely assess the quality of their code, and this can have potential negative consequences for end-users. To this end, we assessed the code quality of 16 highly cited and compute-intensive tools mainly written in C/C++ (e.g., {MrBayes}, {MAFFT}, {SweepFinder}, etc.) and {JAVA} ({BEAST}) from the broader area of evolutionary biology that are being routinely used in current data analysis pipelines. Because, the software engineering quality of the tools we analyzed is rather unsatisfying, we provide a list of best practices for improving the quality of existing tools and list techniques that can be deployed for developing reliable, high quality scientific software from scratch. Finally, we also discuss journal as well as science policy and, more importantly, funding issues that need to be addressed for improving software engineering quality as well as ensuring support for developing new and maintaining existing software. Our intention is to raise the awareness of the community regarding software engineering quality issues and to emphasize the substantial lack of funding for scientific software development.},
	pages = {1037--1046},
	number = {5},
	journaltitle = {Molecular Biology and Evolution},
	shortjournal = {Molecular Biology and Evolution},
	author = {Darriba, Diego and Flouri, Tomáš and Stamatakis, Alexandros},
	date = {2018-05-01},
	file = {Full Text PDF:/home/christoph/Zotero/storage/93MHFC4D/Darriba et al. - 2018 - The State of Software for Evolutionary Biology.pdf:application/pdf;Snapshot:/home/christoph/Zotero/storage/P25TZLEA/4828033.html:text/html},
}

@article{shen_investigation_2020,
	title = {An investigation of irreproducibility in maximum likelihood phylogenetic inference},
	volume = {11},
	rights = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-20005-6},
	doi = {10.1038/s41467-020-20005-6},
	abstract = {Phylogenetic trees are essential for studying biology, but their reproducibility under identical parameter settings remains unexplored. Here, we find that 3515 (18.11\%) {IQ}-{TREE}-inferred and 1813 (9.34\%) {RAxML}-{NG}-inferred maximum likelihood ({ML}) gene trees are topologically irreproducible when executing two replicates (Run1 and Run2) for each of 19,414 gene alignments in 15 animal, plant, and fungal phylogenomic datasets. Notably, coalescent-based {ASTRAL} species phylogenies inferred from Run1 and Run2 sets of individual gene trees are topologically irreproducible for 9/15 phylogenomic datasets, whereas concatenation-based phylogenies inferred twice from the same supermatrix are reproducible. Our simulations further show that irreproducible phylogenies are more likely to be incorrect than reproducible phylogenies. These results suggest that a considerable fraction of single-gene {ML} trees may be irreproducible. Increasing reproducibility in {ML} inference will benefit from providing analyses’ log files, which contain typically reported parameters (e.g., program, substitution model, number of tree searches) but also typically unreported ones (e.g., random starting seed number, number of threads, processor type).},
	pages = {6096},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Shen, Xing-Xing and Li, Yuanning and Hittinger, Chris Todd and Chen, Xue-xin and Rokas, Antonis},
	urldate = {2022-01-31},
	date = {2020-11-30},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Phylogenetics, Phylogeny},
	file = {Full Text PDF:/home/christoph/Zotero/storage/M2Q8A8ET/Shen et al. - 2020 - An investigation of irreproducibility in maximum l.pdf:application/pdf;Snapshot:/home/christoph/Zotero/storage/XR9S4IEW/s41467-020-20005-6.html:text/html},
}

@book{scornavacca_phylogenetics_2020,
	title = {Phylogenetics in the Genomic Era},
	url = {https://hal.archives-ouvertes.fr/hal-02535070},
	pagetotal = {p.p. 1-568},
	publisher = {No commercial publisher {\textbar} Authors open access book},
	author = {Scornavacca, Celine and Delsuc, Frédéric and Galtier, Nicolas},
	editor = {Scornavacca, Celine and Delsuc, Frédéric and Galtier, Nicolas},
	date = {2020},
	file = {Scornavacca et al. - 2020 - Phylogenetics in the Genomic Era.pdf:/home/christoph/Zotero/storage/HSRHX6HQ/Scornavacca et al. - 2020 - Phylogenetics in the Genomic Era.pdf:application/pdf},
}

@incollection{stamatakis_efficient_2020,
	title = {Efficient maximum likelihood tree building methods},
	pages = {1.2:1 -- 1.2:18},
	booktitle = {Phylogenetics in the Genomic Era},
	author = {Stamatakis, Alexandros and Kozlov, Alexey M},
	date = {2020},
}

@article{roch_short_2006,
	title = {A short proof that phylogenetic tree reconstruction by maximum likelihood is hard},
	volume = {3},
	issn = {1557-9964},
	doi = {10.1109/TCBB.2006.4},
	abstract = {Maximum likelihood is one of the most widely used techniques to infer evolutionary histories. Although it is thought to be intractable, a proof of its hardness has been lacking. Here, we give a short proof that computing the maximum likelihood tree is {NP}-hard by exploiting a connection between likelihood and parsimony observed by Tuffley and Steel},
	pages = {92--94},
	number = {1},
	journaltitle = {{IEEE}/{ACM} Transactions on Computational Biology and Bioinformatics},
	author = {Roch, S.},
	date = {2006-01},
	note = {Conference Name: {IEEE}/{ACM} Transactions on Computational Biology and Bioinformatics},
	keywords = {Phylogeny, Analysis of algorithms and problem complexity, biology and genetics., Biology computing, Evolution (biology), Genetics, History, Probability, probability and statistics, Sequences, Statistics, Steel, Systematics},
	file = {Eingereichte Version:/home/christoph/Zotero/storage/28FTA4UC/Roch - 2006 - A short proof that phylogenetic tree reconstructio.pdf:application/pdf;IEEE Xplore Abstract Record:/home/christoph/Zotero/storage/5RBPTKXK/1588849.html:text/html},
}

@article{goldberg_what_1991,
	title = {What every computer scientist should know about floating-point arithmetic},
	volume = {23},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/103162.103163},
	doi = {10.1145/103162.103163},
	abstract = {Floating-point arithmetic is considered as esoteric subject by many people. This is rather surprising, because floating-point is ubiquitous in computer systems: Almost every language has a floating-point datatype; computers from {PCs} to supercomputers have floating-point accelerators; most compilers will be called upon to compile floating-point algorithms from time to time; and virtually every operating system must respond to floating-point exceptions such as overflow. This paper presents a tutorial on the aspects of floating-point that have a direct impact on designers of computer systems. It begins with background on floating-point representation and rounding error, continues with a discussion of the {IEEE} floating point standard, and concludes with examples of how computer system builders can better support floating point.},
	pages = {5--48},
	number = {1},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Goldberg, David},
	date = {1991-03},
	langid = {english},
	file = {Goldberg - 1991 - What every computer scientist should know about fl.pdf:/home/christoph/Zotero/storage/GF7EXIF9/Goldberg - 1991 - What every computer scientist should know about fl.pdf:application/pdf},
}

@incollection{noauthor_isoiec_nodate,
	title = {{ISO}/{IEC} {DIS} 14882:2020(E)},
	pages = {1125},
	file = {fulltext.pdf:/home/christoph/Zotero/storage/7M3K32I5/fulltext.pdf:application/pdf},
}

@article{dolbeau_theoretical_2018,
	title = {Theoretical peak {FLOPS} per instruction set: a tutorial},
	volume = {74},
	issn = {0920-8542, 1573-0484},
	url = {http://link.springer.com/10.1007/s11227-017-2177-5},
	doi = {10.1007/s11227-017-2177-5},
	shorttitle = {Theoretical peak {FLOPS} per instruction set},
	abstract = {Traditionally, evaluating the theoretical peak performance of a {CPU} in {FLOPS} (ﬂoating-point operations per second) was merely a matter of multiplying the frequency by the number of ﬂoating-point instructions per cycle. Today however, {CPUs} have features such as vectorization, fused multiply-add, hyperthreading, and “turbo” mode. In this tutorial, we look into this theoretical peak for recent fully featured Intel {CPUs} and other hardware, taking into account not only the simple absolute peak, but also the relevant instruction sets, encoding and the frequency scaling behaviour of modern hardware.},
	pages = {1341--1377},
	number = {3},
	journaltitle = {The Journal of Supercomputing},
	shortjournal = {J Supercomput},
	author = {Dolbeau, Romain},
	date = {2018-03},
	langid = {english},
	file = {Dolbeau - 2018 - Theoretical peak FLOPS per instruction set a tuto.pdf:/home/christoph/Zotero/storage/A95NASNM/Dolbeau - 2018 - Theoretical peak FLOPS per instruction set a tuto.pdf:application/pdf},
}

@inproceedings{arteaga_designing_2014,
	title = {Designing Bit-Reproducible Portable High-Performance Applications},
	doi = {10.1109/IPDPS.2014.127},
	abstract = {Bit-reproducibility has many advantages in the context of high-performance computing. Besides simplifying and making more accurate the process of debugging and testing the code, it can allow the deployment of applications on heterogeneous systems, maintaining the consistency of the computations. In this work we analyze the basic operations performed by scientific applications and identify the possible sources of non-reproducibility. In particular, we consider the tasks of evaluating transcendental functions and performing reductions using non-associative operators. We present a set of techniques to achieve reproducibility and we propose improvements over existing algorithms to perform reproducible computations in a portable way, at the same time obtaining good performance and accuracy. By applying these techniques to more complex tasks we show that bit-reproducibility can be achieved on a broad range of scientific applications.},
	eventtitle = {2014 {IEEE} 28th International Parallel and Distributed Processing Symposium},
	pages = {1235--1244},
	booktitle = {2014 {IEEE} 28th International Parallel and Distributed Processing Symposium},
	author = {Arteaga, Andrea and Fuhrer, Oliver and Hoefler, Torsten},
	date = {2014-05},
	note = {{ISSN}: 1530-2075},
	keywords = {Computational modeling, reproducibility, Accuracy, Computer architecture, determinism, Graphics processing units, {IEEE}-754 standard, Libraries, Meteorology, parallelism, Standards},
	file = {IEEE Xplore Abstract Record:/home/christoph/Zotero/storage/HC37KX4S/6877351.html:text/html;IEEE Xplore Full Text PDF:/home/christoph/Zotero/storage/VDAMT8PC/Arteaga et al. - 2014 - Designing Bit-Reproducible Portable High-Performan.pdf:application/pdf},
}

@article{budimlic_deterministic_nodate,
	title = {Deterministic Reductions in an Asynchronous Parallel Language},
	abstract = {Reduction operations are a common and important feature in many parallel programming models. In this paper, we present a new reduction construct for Concurrent Collections ({CnC}). {CnC} is a deterministic, asynchronous parallel programming model in which data production and reduction can overlap. While reductions are most frequently incorporated in synchronous contexts where all data is available before parallel reduction begins, our solution works for the asynchronous {CnC} model. We retain the determinism of the {CnC} parallel programming model while providing an efﬁcient high-level construct for specifying reductions.},
	pages = {6},
	author = {Budimlic, Zoran and Burke, Michael and Knobe, Kathleen and Newton, Ryan and Peixotto, David and Sarkar, Vivek and Westbrook, Edwin},
	langid = {english},
	file = {Budimlic et al. - Deterministic Reductions in an Asynchronous Parall.pdf:/home/christoph/Zotero/storage/J3JG4GZR/Budimlic et al. - Deterministic Reductions in an Asynchronous Parall.pdf:application/pdf},
}

@inproceedings{hunold_reproducible_2014,
	location = {Kyoto Japan},
	title = {Reproducible {MPI} Micro-Benchmarking Isn't As Easy As You Think},
	isbn = {978-1-4503-2875-3},
	url = {https://dl.acm.org/doi/10.1145/2642769.2642785},
	doi = {10.1145/2642769.2642785},
	abstract = {The Message Passing Interface ({MPI}) is the prevalent programming model for supercomputers. Optimizing the performance of individual {MPI} functions is therefore of great interest for the {HPC} community. However, a fair comparison of diﬀerent algorithms and implementations requires a statistically sound analysis. It is often overlooked that the time to complete an {MPI} communication function does not only depend on internal factors such as the algorithm but also on external factors such as the system noise. Most noise produced by the system is uncontrollable without changing the software stack, e.g., the memory allocation method used by the operating system. Possibly controllable factors have not yet been identiﬁed as such in this context. We investigate several possible factors—which have been discovered in other microbenchmarks—whether they have a signiﬁcant eﬀect on the execution time of {MPI} functions. We experimentally and statistically show that results obtained with other common benchmarking methods for {MPI} functions can be misleading when comparing alternatives. To overcome these issues, we explain how to carefully design {MPI} micro-benchmarking experiments and how to make a fair, statistically sound comparison of {MPI} implementations.},
	eventtitle = {{EuroMPI}/{ASIA} '14: 21st European {MPI} Users' Group Meeting},
	pages = {69--76},
	booktitle = {Proceedings of the 21st European {MPI} Users' Group Meeting},
	publisher = {{ACM}},
	author = {Hunold, Sascha and Carpen-Amarie, Alexandra and Träff, Jesper Larsson},
	urldate = {2022-03-29},
	date = {2014-09-09},
	langid = {english},
	file = {Hunold et al. - 2014 - Reproducible MPI Micro-Benchmarking Isn't As Easy .pdf:/home/christoph/Zotero/storage/79XRG3EY/Hunold et al. - 2014 - Reproducible MPI Micro-Benchmarking Isn't As Easy .pdf:application/pdf},
}

@article{diethelm_limits_2012,
	title = {The Limits of Reproducibility in Numerical Simulation},
	volume = {14},
	issn = {1558-366X},
	doi = {10.1109/MCSE.2011.21},
	abstract = {Modern computational simulation's increasing and mainly speed-oriented use of {HPC} systems often conflicts with the goal of making research reproducible. Indeed, the simulations that result from {HPC} use often behave reproducibly in only a limited way As a discussion of this phenomenon's technical background describes, the problems entailed will be very difficult to overcome.},
	pages = {64--72},
	number = {1},
	journaltitle = {Computing in Science Engineering},
	author = {Diethelm, Kai},
	date = {2012-01},
	note = {Conference Name: Computing in Science Engineering},
	keywords = {Computational modeling, High performance computing, numerical analysis, Numerical simulation, Parallel algorithms, Parallel processing, scientific computing, Scientific computing},
	file = {IEEE Xplore Abstract Record:/home/christoph/Zotero/storage/99VD42Q9/5719578.html:text/html;IEEE Xplore Full Text PDF:/home/christoph/Zotero/storage/VR4ESM62/Diethelm - 2012 - The Limits of Reproducibility in Numerical Simulat.pdf:application/pdf},
}

@inproceedings{chapp_need_2015,
	title = {On the Need for Reproducible Numerical Accuracy through Intelligent Runtime Selection of Reduction Algorithms at the Extreme Scale},
	doi = {10.1109/CLUSTER.2015.34},
	abstract = {The inherent nondeterminism present in reduction operations on an exascale system, coupled with the nonassociativity of floating-point arithmetic, makes achieving reproducible results difficult or impossible. Work investigating the irreproducibility phenomenon has generally proceeded along one of two veins: (1) development of algorithms that produce reproducible numerical results irrespective of nondeterminism in the reduction tree and (2) study of the system-level factors that induce nondeterminism. Our work builds on the latter and unveils the power of mathematical methods to mitigate error propagation at the exascale. We focus on floating-point error accumulation over global summations where enforcing any reduction order is expensive or impossible. We model parallel summations with reduction trees and identify those parameters that can be used to estimate the reduction's sensitivity to variability in the reduction tree. We assess the impact of these parameters on the ability of different reduction methods to successfully mitigate errors. Our results illustrate the pressing need for intelligent runtime selection of reduction operators that ensure a given degree of reproducible accuracy.},
	eventtitle = {2015 {IEEE} International Conference on Cluster Computing},
	pages = {166--175},
	booktitle = {2015 {IEEE} International Conference on Cluster Computing},
	author = {Chapp, Dylan and Johnston, Travis and Taufer, Michela},
	date = {2015-09},
	note = {{ISSN}: 2168-9253},
	keywords = {Accuracy, composite precision summation, Hardware, Kahan's compensated summation, Prediction algorithms, Prerounded summation, reduction tree, Sensitivity, Shape, Software algorithms, Standards},
	file = {IEEE Xplore Abstract Record:/home/christoph/Zotero/storage/T9CINC4E/7307581.html:text/html;IEEE Xplore Full Text PDF:/home/christoph/Zotero/storage/WBSXACQI/Chapp et al. - 2015 - On the Need for Reproducible Numerical Accuracy th.pdf:application/pdf},
}

@article{wiesenberger_reproducibility_2019,
	title = {Reproducibility, accuracy and performance of the Feltor code and library on parallel computer architectures},
	volume = {238},
	issn = {00104655},
	url = {http://arxiv.org/abs/1807.01971},
	doi = {10.1016/j.cpc.2018.12.006},
	abstract = {Feltor is a modular and free scientiﬁc software package. It allows developing platform independent code that runs on a variety of parallel computer architectures ranging from laptop {CPUs} to multi-{GPU} distributed memory systems. Feltor consists of both a numerical library and a collection of application codes built on top of the library. Its main target are two- and three-dimensional drift- and gyro-ﬂuid simulations with discontinuous Galerkin methods as the main numerical discretization technique.},
	pages = {145--156},
	journaltitle = {Computer Physics Communications},
	shortjournal = {Computer Physics Communications},
	author = {Wiesenberger, Matthias and Einkemmer, Lukas and Held, Markus and Gutierrez-Milla, Albert and Saez, Xavier and Iakymchuk, Roman},
	date = {2019-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.01971},
	keywords = {Physics - Computational Physics},
	file = {Wiesenberger et al. - 2019 - Reproducibility, accuracy and performance of the F.pdf:/home/christoph/Zotero/storage/YEZT2NSW/Wiesenberger et al. - 2019 - Reproducibility, accuracy and performance of the F.pdf:application/pdf},
}